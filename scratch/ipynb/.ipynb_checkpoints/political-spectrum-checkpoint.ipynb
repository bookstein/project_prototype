{
 "metadata": {
  "name": "",
  "signature": "sha256:8987c7e7e966ed32a681e3f4ad853273bd946b64a462a104d659858c978925e9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import simplejson as json\n",
      "import os\n",
      "import string\n",
      "\n",
      "import numpy as np\n",
      "import tweepy\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
      "from sklearn import cross_validation\n",
      "from sklearn import metrics\n",
      "\n",
      "import model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TEXT = []\n",
      "LABELS = []\n",
      "\n",
      "data = model.Status.get_all_statuses()\n",
      "for status in data:\n",
      "    TEXT.append(status.text.lower())\n",
      "    #label 'p' for political\n",
      "    if status.label == \"libs\" or status.label == \"cons\":\n",
      "        LABELS.append(\"p\")\n",
      "    else:\n",
      "        LABELS.append(\"np\")\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"TEXT LEN:\", len(TEXT)\n",
      "print \"LABELS LEN:\", len(LABELS)\n",
      "print TEXT[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TEXT LEN: 23977\n",
        "LABELS LEN: 23977\n",
        "rt @jjauthor: apparently with \"legalization first with no secure border\"-how many terrorists do democrats want to let into america? #tcot #\u2026\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ENGLISH_STOP_WORDS = [\n",
      "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
      "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
      "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
      "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
      "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
      "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
      "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
      "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
      "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
      "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
      "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
      "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\",\n",
      "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
      "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
      "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
      "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
      "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
      "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
      "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
      "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
      "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
      "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
      "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
      "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
      "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
      "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
      "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
      "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
      "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
      "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
      "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
      "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
      "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
      "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
      "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
      "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
      "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
      "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
      "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
      "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
      "    \"yourselves\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stops = ['http', 'rt']\n",
      "stops.extend(ENGLISH_STOP_WORDS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "makeVector = TfidfVectorizer(analyzer=\"word\", stop_words=stops)\n",
      "print makeVector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TfidfVectorizer(analyzer='word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words=['http', 'rt', 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', '...ill', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves'],\n",
        "        strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "        vocabulary=None)\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create vector from raw documents (X)\n",
      "X = makeVector.fit_transform(TEXT)\n",
      "print X.shape\n",
      "# print X\n",
      "\n",
      "# create numpy array of labels\n",
      "# numpy arrays = n-dimensional array with collection of items all the same type. Homogenous.\n",
      "y = np.array(LABELS)\n",
      "print y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(23977, 44102)\n",
        "(23977,)\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_fraction_np(text_list, label_list):\n",
      "    \"\"\"\n",
      "    Get fraction of training data that is associated with \"np\" (nonpolitical)\n",
      "    label.\n",
      "    \"\"\"\n",
      "    # get number of nonpolitical\n",
      "    np_list = [label for label in label_list if label == 'np']\n",
      "    \n",
      "#     print len(text_list)\n",
      "#     print len(np_list)\n",
      "    \n",
      "    return float(len(np_list))/float(len(text_list))\n",
      "\n",
      "print get_fraction_np(TEXT, LABELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.749760186846\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = BernoulliNB(fit_prior=True,alpha=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(X, y)\n",
      "# train classifier to recognize all tweets as 'political' (single label)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Kfolds = 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = cross_validation.StratifiedKFold(y, Kfolds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precision = []\n",
      "recall = []\n",
      "\n",
      "for train, test in cv:\n",
      "    X_train = X[train]\n",
      "    X_test = X[test]\n",
      "    y_train = y[train]\n",
      "    y_test = y[test]\n",
      "\n",
      "    clf.fit(X_train, y_train)\n",
      "\n",
      "    y_hat = clf.predict(X_test)\n",
      "\n",
      "    p,r,f1_score,support = metrics.precision_recall_fscore_support(y_test, y_hat)\n",
      "\n",
      "    precision.append(p[1])\n",
      "    recall.append(r[1])\n",
      "\n",
      "print 'avg precision:',np.average(precision), '+/-', np.std(precision)\n",
      "print 'avg recall:', np.average(recall), '+/-', np.std(recall)\n",
      "print 'f1 measure', f1_score\n",
      "\n",
      "print \"clf: \", clf\n",
      "print \"cv: \", cv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg precision: 0.961076789656 +/- 0.0295156019763\n",
        "avg recall: 0.9965 +/- 0.0023213980462\n",
        "f1 measure [ 0.98277809  0.95131684]\n",
        "clf:  BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "cv:  sklearn.cross_validation.StratifiedKFold(labels=['p' 'p' 'p' ..., 'np' 'np' 'np'], n_folds=5, shuffle=False, random_state=None)\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweet1 = 'This Veterans Day, join me at #TheConcertForValor, a free event at the #NationalMall in Washington DC https://www.youtube.com/watch?v=U8NtbVL-CKM \u2026'\n",
      "tweet2 = 'Here is our piece on Iraqi and Afghani translators from last night. Buckle up. http://www.youtube.com/watch?v=QplQL5eAxlY&list=UU3XTzVzaHQEd30rQbuvCtTQ&index=1 \u2026'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# concatenate tweets to make a 'timeline' -- \n",
      "# roughly equivalent to rating each tweet separately and then averaging based on this example data\n",
      "timeline = [tweet1 + tweet2]\n",
      "sample = makeVector.transform(timeline)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sample"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 42774)\t0.389390998278\n",
        "  (0, 41917)\t0.491864155234\n",
        "  (0, 40876)\t0.262111350587\n",
        "  (0, 40860)\t0.212685890336\n",
        "  (0, 40127)\t0.188333431983\n",
        "  (0, 29711)\t0.233733298784\n",
        "  (0, 27262)\t0.158179550399\n",
        "  (0, 23312)\t0.215742907586\n",
        "  (0, 20791)\t0.180170868256\n",
        "  (0, 18443)\t0.1308084863\n",
        "  (0, 15358)\t0.141923154717\n",
        "  (0, 13914)\t0.205219203363\n",
        "  (0, 11189)\t0.22320959456\n",
        "  (0, 11152)\t0.132524595934\n",
        "  (0, 9705)\t0.389390998278\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# class prediction\n",
      "prediction = clf.predict(sample)\n",
      "print prediction\n",
      "\n",
      "# probabilities of belonging to class == SCORE!\n",
      "probs = clf.predict_proba(sample)\n",
      "print probs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['np']\n",
        "[[ 0.72314243  0.27685757]]\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print clf.predict_log_proba(makeVector.transform([tweet1]))\n",
      "# print clf.predict_log_proba(makeVector.transform([tweet2]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.feature_count_ # for Naive Bayes only, not LogReg\n",
      "# feature_count_ is number of samples encountered for each (class, feature) during fitting"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 103,
       "text": [
        "array([[ 38.,  57.,   0., ...,   1.,   2.,   4.],\n",
        "       [  0.,  31.,   0., ...,   0.,   0.,   0.]])"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "probs=clf.feature_log_prob_[1]\n",
      "print probs\n",
      "# feature_log_prob_ is empirical log probability of features given a class, P(x_i|y).\n",
      "# creates vector of probabilities by feature"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-8.47678778 -5.01105187 -8.47678778 ..., -8.47678778 -8.47678778\n",
        " -8.47678778]\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features=makeVector.get_feature_names()\n",
      "len(features)\n",
      "#list of features (words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'00', u'000', u'000_nico_000', u'001', u'003', u'004', u'005', u'008', u'00pgl6qik4', u'00pm', u'00qezoiipe', u'00tsoff0iw', u'01', u'010', u'012', u'012tc2zper', u'014', u'015', u'016', u'017', u'018', u'019', u'01dspx94rq', u'01ete1zkvh', u'01li8rkbot', u'01pm', u'02', u'020', u'0208', u'021', u'021mk1gbdf', u'024', u'025', u'026', u'027', u'028', u'02qpdaj9ui', u'03', u'030', u'031', u'032', u'034', u'035', u'036', u'037', u'038', u'039', u'03l9bpe01b', u'04', u'040', u'041', u'042', u'0430yes', u'044', u'045', u'047', u'048', u'049pcnj7zg', u'04dwyctxaj', u'04onzd4tg6', u'05', u'050', u'0500447344', u'050ft6yui3', u'051', u'053', u'054', u'055', u'0551882263', u'056', u'057', u'059', u'05la9brwoe', u'06', u'060', u'061', u'061005', u'062', u'063', u'064', u'065', u'066', u'067', u'068', u'069', u'06mpixvngb', u'07', u'070', u'071', u'073', u'074', u'0742', u'075', u'076', u'07bb7ex7rg', u'07ctw', u'08', u'080', u'082', u'084']\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(zip(probs,features), reverse=True)[:10]\n",
      "# zip together features and their probabilities, sort in reverse order (descending)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 106,
       "text": [
        "[(-0.73135216650681834, u'tcot'),\n",
        " (-1.0574071938625069, u'obama'),\n",
        " (-1.2469489986299491, u'p2'),\n",
        " (-1.8098309843519926, u'immigration'),\n",
        " (-2.1224177359838485, u'president'),\n",
        " (-2.1723389743592181, u'pjnet'),\n",
        " (-2.1963919378210042, u'immigrationaction'),\n",
        " (-2.2905791528807056, u'amnesty'),\n",
        " (-2.3968545816856093, u'gop'),\n",
        " (-2.4223484305118284, u'amp')]"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# zip together feature log probs with NONPOLITICAL features, in reverse order (descending)\n",
      "np_features = sorted(zip(clf.feature_log_prob_[0],features), key=lambda f: f[0], reverse=True)[:10]\n",
      "# zip together feature log probs with POLITICAL features\n",
      "p_features = sorted(zip(clf.feature_log_prob_[1],features), key=lambda f: f[0], reverse=True)[:10]\n",
      "\n",
      "print np_features\n",
      "print \"\\n\\n\"\n",
      "print p_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(-1.4353070194497697, u'gameinsight'), (-1.5098644096147407, u'love'), (-1.5605285753247298, u'nowplaying'), (-1.7148447769598638, u'win'), (-1.8198191176756442, u'ff'), (-1.8762964099090587, u've'), (-2.183690328484972, u'amp'), (-2.2507010387679323, u'android'), (-2.2686835413183646, u'androidgames'), (-2.3454833051377975, u'ipadgames')]\n",
        "\n",
        "\n",
        "\n",
        "[(-0.73135216650681834, u'tcot'), (-1.0574071938625069, u'obama'), (-1.2469489986299491, u'p2'), (-1.8098309843519926, u'immigration'), (-2.1224177359838485, u'president'), (-2.1723389743592181, u'pjnet'), (-2.1963919378210042, u'immigrationaction'), (-2.2905791528807056, u'amnesty'), (-2.3968545816856093, u'gop'), (-2.4223484305118284, u'amp')]\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}