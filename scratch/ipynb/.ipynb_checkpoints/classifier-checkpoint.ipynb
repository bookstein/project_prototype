{
 "metadata": {
  "name": "",
  "signature": "sha256:001a63c1dd5272474a3527055a6fb371ddac69b330b511c017d74ca88777e8de"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import simplejson as json\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
      "from sklearn import cross_validation\n",
      "from sklearn import metrics\n",
      "\n",
      "import model\n",
      "\n",
      "import os\n",
      "\n",
      "import tweepy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TEXT = []\n",
      "LABELS = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get all records (labeled tweets from #tcot feed and #p2 feed) from database\n",
      "data = model.Status.get_all_statuses()\n",
      "for status in data:\n",
      "    TEXT.append(status.text)\n",
      "    LABELS.append(status.label)\n",
      "\n",
      "print len(TEXT)\n",
      "print len(LABELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6000\n",
        "6000\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"TEXT:\", TEXT[1], \"LABEL:\", LABELS[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TEXT: RT @Shawtwo: Editorial Cartoon: What The Frack? http://t.co/6YX99kYJSt #StopRush http://t.co/kWfPEE22bA #UniteBlue #p2 LABEL: libs\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_fraction_cons(text_list, label_list):\n",
      "    \"\"\"\n",
      "    Get fraction of training data that is associated with \"cons\" (conservative)\n",
      "    label.\n",
      "    \"\"\"\n",
      "    total_length = len(text_list)\n",
      "    # print \"total length\", total_length\n",
      "    cons_list = [label for label in label_list if label == 'cons']\n",
      "    # print cons_list\n",
      "    cons_fraction = len([label for label in label_list if label == 'cons'])\n",
      "    # print \"fraction of data that is conservative: \", cons_fraction, \" out of \", total_length\n",
      "    return cons_fraction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_fraction_cons(TEXT, LABELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "3000"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# makes vector\n",
      "makeVector = TfidfVectorizer(analyzer=\"word\", stop_words=\"english\")\n",
      "print makeVector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TfidfVectorizer(analyzer='word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "        vocabulary=None)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create vector from raw documents (X)\n",
      "X = makeVector.fit_transform(TEXT)\n",
      "# create numpy array\n",
      "y = np.array(LABELS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def init_and_train_classifier(X, y, Kfolds):\n",
      "    \"\"\"\n",
      "    Instantiates and trains a classifier.\n",
      "\n",
      "    Parameters:\n",
      "    ----------\n",
      "    Matrix of documents and features\n",
      "    List of labels (as numpy array)\n",
      "    Number of stratified folds into which to divide all labelled data.\n",
      "\n",
      "    Output:\n",
      "    -------\n",
      "    Trained classifier\n",
      "    \"\"\"\n",
      "\n",
      "    clf = MultinomialNB()\n",
      "\n",
      "    cv = cross_validation.StratifiedKFold(y, Kfolds)\n",
      "\n",
      "    precision = []\n",
      "    recall = []\n",
      "\n",
      "    for train, test in cv:\n",
      "        X_train = X[train]\n",
      "        X_test = X[test]\n",
      "        y_train = y[train]\n",
      "        y_test = y[test]\n",
      "\n",
      "        clf.fit(X_train, y_train)\n",
      "\n",
      "        y_hat = clf.predict(X_test)\n",
      "\n",
      "        p,r,f1_score,support = metrics.precision_recall_fscore_support(y_test, y_hat)\n",
      "\n",
      "        precision.append(p[1])\n",
      "        recall.append(r[1])\n",
      "\n",
      "    print 'avg precision:',np.average(precision), '+/-', np.std(precision)\n",
      "    print 'avg recall:', np.average(recall), '+/-', np.std(recall)\n",
      "    print 'f1 measure', f1_score\n",
      "\n",
      "    print \"clf: \", clf\n",
      "    print \"cv: \", cv\n",
      "\n",
      "    return clf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = init_and_train_classifier(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg precision: 0.958960134318 +/- 0.0123391328508\n",
        "avg recall: 0.922 +/- 0.0231516738056\n",
        "f1 measure [ 0.91572123  0.91080617]\n",
        "clf:  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "cv:  sklearn.cross_validation.StratifiedKFold(labels=[u'libs' u'libs' u'libs' ..., u'libs' u'libs' u'libs'], n_folds=5, shuffle=False, random_state=None)\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connect to Twitter API\n",
      "def connect_to_API():\n",
      "    \"\"\"\n",
      "    Create instance of tweepy API class with OAuth keys and tokens.\n",
      "    \"\"\"\n",
      "    # initialize tweepy api object with auth, OAuth\n",
      "    TWITTER_API_KEY=os.environ.get('TWITTER_API_KEY')\n",
      "    TWITTER_SECRET_KEY=os.environ.get('TWITTER_SECRET_KEY')\n",
      "    TWITTER_ACCESS_TOKEN=os.environ.get('TWITTER_ACCESS_TOKEN')\n",
      "    TWITTER_SECRET_TOKEN=os.environ.get('TWITTER_SECRET_TOKEN')\n",
      "\n",
      "    auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_SECRET_KEY, secure=True)\n",
      "    auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_SECRET_TOKEN)\n",
      "    api = tweepy.API(auth, cache=None) #removed wait_on_rate_limit=True, wait_on_rate_limit_notify=True\n",
      "\n",
      "    return api"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get user timeline by user id\n",
      "def get_timeline(api, uid, count):\n",
      "    \"\"\"Get n number of tweets by passing in user id and number of statuses.\n",
      "        If user has protected tweets, returns [] rather than break the program.\n",
      "    \"\"\"\n",
      "    timeline_list = []\n",
      "    \n",
      "    try:\n",
      "        timeline = tweepy.Cursor(api.user_timeline, id=uid).items(count)\n",
      "        for status in timeline:\n",
      "            timeline_list.append(status)\n",
      "        return timeline_list\n",
      "\n",
      "    except tweepy.TweepError as e:\n",
      "        print e.message[0][\"error\"]\n",
      "        return []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get all text from user's timeline, store in list\n",
      "def extract_text(statuses):\n",
      "    \"\"\"\n",
      "    Extract text from a given twitter user's timeline and\n",
      "    append to list.\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    Statuses: the new twitter user's timeline, a python object\n",
      "    containing status objects, as tweepy object\n",
      "\n",
      "    Output:\n",
      "    -------\n",
      "    List of statuses' text fields.\n",
      "    \"\"\"\n",
      "    text_list = []\n",
      "\n",
      "    for status in statuses:\n",
      "        status = status._json\n",
      "        text = status[\"text\"]\n",
      "        text_list.append(text)\n",
      "\n",
      "    return text_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tweets_by_query(api, query, max_tweets):\n",
      "    \"\"\"\n",
      "    Get {max_tweets} tweets labeled by a particular hashtag {query} from Twitter Search API\n",
      "\n",
      "    Parameters:\n",
      "    ----------\n",
      "    Api: the Tweepy API instance.\n",
      "    Query: hashtag to search for, as a string, prefixed by \"#\"\n",
      "    Max_tweets: the total number of tweets requested.\n",
      "\n",
      "    Output:\n",
      "    ------\n",
      "    List of JSON statuses\n",
      "\n",
      "    Note:\n",
      "    ----\n",
      "    Per 15 minutes, app can make 450 requests.\n",
      "    Number of tweets per page defaults to 15. \"Count\" maximum is 100.\n",
      "\n",
      "    \"\"\"\n",
      "    searched_tweets = []\n",
      "\n",
      "    max_id = -1\n",
      "\n",
      "    while len(searched_tweets) < max_tweets:\n",
      "        count = max_tweets - len(searched_tweets)\n",
      "        try:\n",
      "            new_tweets = api.search(q=query, count=count, include_entities=True,\n",
      "             max_id=str(max_id - 1))\n",
      "            if not new_tweets:\n",
      "                break\n",
      "\n",
      "            for tweet in new_tweets:\n",
      "                tweet = tweet._json\n",
      "                searched_tweets.append(tweet)\n",
      "\n",
      "            max_id = new_tweets[-1].id\n",
      "            since_id = new_tweets[0].id\n",
      "\n",
      "            print \"max_id:\", max_id\n",
      "            print \"since_id\", since_id\n",
      "\n",
      "        except tweepy.TweepError as e:\n",
      "            # depending on TweepError.code, one may want to retry or wait\n",
      "            # to keep things simple, we will give up on an error\n",
      "            break\n",
      "    return searched_tweets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "api = connect_to_API()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tcot = model.Status.get_cons_statuses()\n",
      "tcot_list = []\n",
      "print len(tcot)\n",
      "for status in tcot:\n",
      "    tcot_list.append(status.text)\n",
      "\n",
      "johnoliver = get_timeline(api, \"iamjohnoliver\", 100)\n",
      "text = extract_text(johnoliver)\n",
      "print len(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3000\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample = makeVector.transform(tcot_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction = clf.predict(sample)\n",
      "print prediction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'cons' u'cons' u'cons' u'cons' u'cons' u'cons' u'cons' u'cons' u'cons'\n",
        " u'cons']\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score = 0\n",
      "for label in prediction:\n",
      "    if label == \"cons\":\n",
      "        score = score + 0\n",
      "    else:\n",
      "        score = score + 1\n",
      "print score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "probs=clf.feature_log_prob_[1]\n",
      "len(probs)\n",
      "\n",
      "probability= clf.predict_proba(X)\n",
      "print probability"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  3.87542203e-02   9.61245780e-01]\n",
        " [  2.17727032e-02   9.78227297e-01]\n",
        " [  4.21251450e-01   5.78748550e-01]\n",
        " ..., \n",
        " [  4.78700042e-04   9.99521300e-01]\n",
        " [  2.95526995e-03   9.97044730e-01]\n",
        " [  2.95526995e-03   9.97044730e-01]]\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score = clf.score(X, y)\n",
      "print score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.975833333333\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = makeVector.get_feature_names()\n",
      "len(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 109,
       "text": [
        "10590"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(zip(probs,features), reverse=True)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 110,
       "text": [
        "[(-4.6836306798805785, u'p2'),\n",
        " (-4.8723481404690947, u'http'),\n",
        " (-5.2765323364817203, u'rt'),\n",
        " (-5.3846870387494326, u'uniteblue'),\n",
        " (-5.6808890627490589, u'stoprush'),\n",
        " (-6.019668593116057, u'kwfpee22ba'),\n",
        " (-6.0468542892780919, u'obama'),\n",
        " (-6.0978074296387383, u'immigration'),\n",
        " (-6.1290488927787656, u'news'),\n",
        " (-6.2115412253470348, u'republicans')]"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}